{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Notations\n",
    "\n",
    "* $x^{(i)}$ input variables/features and  the space of all such $x^{(i)}$ is $X$\n",
    "* $y^{(i)}$ output/target variable that we are trying to predict and the space of all such $y^{(i)}$ is $Y$\n",
    "* A pair $(x^{(i)},y^{(i)})$ is called a training example and the dataset we will be using to learn - a list of $n$ training examples $ \\{(x^{(i)},y^{(i)});i=1,2,...,n\\} $ - is called a training set\n",
    "* the superscript $“(i)”$ in the notation is simply an index into the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Problem\n",
    "\n",
    "\n",
    "* The goal is, given a training set, to learn a function $ h: X \\mapsto Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. For historical reasons, this function h is called a hypothesis.\n",
    "\n",
    "* If  the target variable is continous ,the learning problem is **regression** \n",
    "* When y can take on only a small number of discrete values,the learning problem is **classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to know functions/hypotheses $h$.An initial choice can be approximating $y$ as a linear function of $x$: $$ h_{\\theta}(x)=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}$$ (if we choose only two features)\n",
    "\n",
    "* $\\theta_{i}$'s are **parameters/weight** parametrizing the space of linear functions mapping from $X$ to $Y$.\n",
    "* Using the convention $x_{0}=1$(**Intercept term**),so that $$h(x)=\\sum_{i=0}^{d}\\theta_{i}x_{i}=\\theta^{T}x,$$\n",
    "$\\theta$ and $x$ are vectors and $d$ is the number of input variables (not counting $x_{0}$)\n",
    "\n",
    "* For a training set,we have to pick or  learn the parameters $\\theta,$ so that we can predict $y$.\n",
    "* One reasonable method seems to be to make $h(x)$ cloase to y,for the training examples.To formalize this, we will define a function that measures, for each value of the θ’s, how close the $h(x^{(i)})'$ s are to the corrsponding $y^{(i)}$'s.\n",
    "\n",
    "* We  define **Cost/Loss** function: $$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{n}(h_{\\theta}(x^{(i)})-y^{(i)})^{2},$$this is the least squares-cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given:  $(x^{(i)},y^{(i)}),$ $i=1,...,n$\n",
    "* Minimize: $J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{n}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}$\n",
    "\n",
    "* We want to choose θ so as to minimize $J(\\theta)$\n",
    "\n",
    "* We can use the gradient descent algorithm that starts with some “initial guess” for $\\theta$, and that repeatedly changes $\\theta$ to make $J(\\theta)$ smaller, until hopefully we converge to a value of\n",
    "$\\theta$ that minimizes $J(\\theta)$.\n",
    "\n",
    "* The **Gradient Descent ** algorithm,which starts from initial $\\theta,$ and repeatedly performs the update: $$\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\partial J( \\theta)}{\\partial \\theta_{j}} ,$$ until some condition is met.\n",
    "This update is simultaneously performed for all values of  $j = 0, . . . , d$\n",
    "\n",
    "* $\\alpha$ is called the learning rate. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of $J$.\n",
    "\n",
    "    * We need to find the partial derivative in the quation,so that we can implement the algorithm.For a single training example $(x,y).$ We have:$$\\frac{\\partial J( \\theta)}{\\partial \\theta_{j}}=\\frac{\\partial}{\\partial \\theta_{j}}\\frac{1}{2}(h_{\\theta}(x)-y)^{2}$$\n",
    "$$=2.\\frac{1}{2}(h_{\\theta}(x)-y).\\frac{\\partial}{\\partial \\theta_{j}}(h_{\\theta}(x)-y)$$\n",
    "$$=(h_{\\theta}(x)-y).\\frac{\\partial}{\\partial \\theta_{j}} (\\sum _{i=0}^{d}\\theta_{i}x_{i}-y)$$\n",
    "$$=(h_{\\theta}(x)-y)x_{j}$$\n",
    "\n",
    "* for a single training example,this gives the update rule:$$\\theta_{j}:=\\theta_{j}+\\alpha(y^{(i)}-h_{\\theta}(x^{(i)}))x_{j}^{(i)},$$this rule is called **LMS**(Least Mean Square)update rule.\n",
    "\n",
    "For a training set,\n",
    "* Repeat until convergence {$$\\theta_{j}:=\\theta_{j}+\\alpha\\sum_{i=1}^{n}(y^{(i)}-h_{\\theta}(x^{(i)}))x_{j}^{(i)}$$ ( for every $j$) .}\n",
    "\n",
    "* $J$ being convex quadratic function,the optimization here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Matrix Picture\n",
    "\n",
    "\n",
    "\n",
    "* Given a set of training examples $(x^{(i)},y^{(i)})$ i.e training set ,define matrix $X$ to be the $m$-by-$n$ matrix ($m$-by-$n+1$,if intercept term is absorbed) that contains the input values of training examples in it's rows:  \n",
    "$$ \\begin{bmatrix} \n",
    "     & {(x^{(1)})^{T}} &  \\\\\n",
    "     & \\vdots & \\\\\n",
    "     &   (x^{(m)})^{T}     &  \n",
    "        \\end{bmatrix} $$\n",
    "    \n",
    "    \n",
    "  * $ \\overrightarrow{\\textbf y}$ be the $m-$ dimensional vector containing the target values from the training set:\n",
    "    $$\\begin{bmatrix} \n",
    "     & y^{(1)} &  \\\\\n",
    "     & \\vdots & \\\\\n",
    "     &   y^{(m)}     &  \n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    " \n",
    " * Since $h_{\\theta}(x^{(i)})=(x^{(i)})^{T}\\theta$,we can verify that\n",
    " $$X \\theta - \\overrightarrow{\\textbf y} = \\begin{bmatrix} \n",
    "     & {(x^{(1)})^{T}}\\theta &  \\\\\n",
    "     & \\vdots & \\\\\n",
    "     &   (x^{(m)})^{T} \\theta    &  \n",
    "    \\end{bmatrix} - \\begin{bmatrix} \n",
    "     & y^{(1)} &  \\\\\n",
    "     & \\vdots & \\\\\n",
    "     &   y^{(m)}     &  \n",
    "         \\end{bmatrix}$$\n",
    "    \n",
    "    $$=\\begin{bmatrix} \n",
    "     & h_{\\theta}(x^{(1)})-y^{(1)} &  \\\\\n",
    "     & \\vdots & \\\\\n",
    "     &  h_{\\theta}(x^{(m)})- y^{(m)}     &  \n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "    \n",
    " $\\frac{1}{2}(X\\theta-\\overrightarrow{\\textbf y})^{T}(X\\theta-\\overrightarrow{\\textbf y})=\\frac{1}{2} \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}=J(\\theta)$ This is the cost function.\n",
    " * To minimize $J$,we have to find the derivatives with respect to $\\theta$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Some matrix derivatives result \n",
    "  $$\\nabla_{A}tr AB = B^{T}$$ <br>\n",
    "  $$\\nabla_{A}\\vert A\\vert=\\vert A\\vert(A^{-1})^{T}$$ <br>\n",
    "  $$\\nabla_{A^{T}}f(A)= (\\nabla_{A}f(A))^{T}$$ <br>\n",
    "  $$\\nabla_{A}trABA^{T}C=CAB+C^{T}AB^{T}$$\n",
    "\n",
    "\n",
    "* Using the last two results \n",
    " $$\\nabla_{A^{T}}trABA^{T}C=B^{T}A^{T}C^{T}+BA^{T}C$$\n",
    " \n",
    "\n",
    "\n",
    "* The cost function $$J(\\theta)=\\frac{1}{2}(X\\theta-\\overrightarrow{ y})^{T}(X\\theta-\\overrightarrow{ y})$$\n",
    " $$\\nabla_{\\theta} J(\\theta)=\\nabla_{\\theta}\\frac{1}{2}(X\\theta-\\overrightarrow{ y})^{T}(X\\theta-\\overrightarrow{ y})$$\n",
    " \n",
    " <br>\n",
    " \n",
    "     $$=\\frac{1}{2}\\nabla_{\\theta}(\\theta^{T}X^{T}X\\theta-\\theta^{T}X^{T}\\overrightarrow{ y}-\\overrightarrow{ y}^{T}X\\theta+\\overrightarrow{ y}^{T}\\overrightarrow{ y})$$\n",
    "     \n",
    "   <br>\n",
    "     \n",
    "     $$=\\frac{1}{2}\\nabla_{\\theta} tr(\\theta^{T}X^{T}X\\theta-\\theta^{T}X^{T}\\overrightarrow{ y}-\\overrightarrow{ y}^{T}X\\theta+\\overrightarrow{ y}^{T}\\overrightarrow{ y})$$\n",
    "   \n",
    "   \n",
    "   \n",
    "   <br>\n",
    "     $$=\\frac{1}{2}\\nabla_{\\theta}(tr \\theta^{T}X^{T}X\\theta-2tr\\overrightarrow{ y}^{T}X\\theta)$$\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\frac{1}{2}(X^{T}X\\theta+X^{T}X\\theta-2X^{T}\\overrightarrow{ y})$$ ,<br>\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $$=X^{T}X\\theta-X^{T}\\overrightarrow{ y}$$\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " * To minimize $J,$we set the derivative to zero and obtain:\n",
    " $$X^{T}X\\theta=X^{T}\\overrightarrow{ y}$$\n",
    " \n",
    " \n",
    " \n",
    " * Thus the value of $\\theta $ that minimizes $J(\\theta)$ is given in closed form by the equation\n",
    " $$\\theta=(X^{T}X)^{-1}X^{T}\\overrightarrow{ y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
